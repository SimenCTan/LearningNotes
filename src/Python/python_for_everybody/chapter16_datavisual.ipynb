{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Python中，**Web Crawler**（网络爬虫）是用于自动访问和提取网页内容的程序。它的工作流程通常可以分为几个主要步骤，结合你提供的图片，我们可以更清晰地理解这个过程。\n",
    "![webcrawler](./assets/webcrawler.png)\n",
    "\n",
    "### 定义\n",
    "Web Crawler 是一种自动化程序，它可以遍历互联网，下载网页并提取信息。爬虫通常用于搜索引擎、数据挖掘和信息收集等应用。\n",
    "\n",
    "### 工作流程\n",
    "1. **调度器（Scheduler）**：\n",
    "   - 爬虫首先由调度器管理，它负责维护待爬取的URL列表。调度器会选择一个URL并将其发送给下载器。\n",
    "\n",
    "2. **下载器（Downloader）**：\n",
    "   - 下载器接收来自调度器的URL，并请求该网页。它会从互联网下载页面内容，包括HTML、文本和其他元数据。\n",
    "\n",
    "3. **提取链接**：\n",
    "   - 下载完成后，爬虫会分析下载的页面，查找其中的链接。这一步骤涉及解析HTML内容，以识别出所有可以访问的URL。\n",
    "\n",
    "4. **更新待爬取列表**：\n",
    "   - 找到的链接会被添加到待爬取的URL列表中，调度器会继续处理这些新链接。\n",
    "\n",
    "5. **存储（Storage）**：\n",
    "   - 下载的网页内容和提取的信息会被存储在数据库或文件系统中，以便后续查询和分析。\n",
    "\n",
    "### 总结\n",
    "这个流程是一个循环的过程，爬虫会不断地从待爬取的列表中取出链接，下载页面，提取信息，并将新的链接加入列表，直到达到预设的停止条件（如爬取的深度或时间限制）。\n",
    "\n",
    "通过Python，开发者可以利用库如 `requests` 和 `BeautifulSoup` 来实现上述功能，创建自己的网络爬虫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
