{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transport control protocol\n",
    "![tcp](./assets/tcp.png)\n",
    "\n",
    "这张图片介绍了传输控制协议（TCP）的基本概念和功能。图片内容包括文字说明和一个堆栈连接（Stack Connections）的示意图。以下是详细解析：\n",
    "\n",
    "### 文字说明部分：\n",
    "1. **Built on top of IP (Internet Protocol)**：\n",
    "   - 这一点说明TCP是建立在IP协议之上的。IP协议负责数据包的传输，但不保证数据包的可靠传输。\n",
    "   \n",
    "2. **Assumes IP might lose some data - stores and retransmits data if it seems to be lost**：\n",
    "   - TCP假设IP可能会丢失一些数据，因此TCP会存储并在检测到数据丢失时重新传输数据，以确保数据的可靠传输。\n",
    "   \n",
    "3. **Handles \"flow control\" using a transmit window**：\n",
    "   - TCP使用传输窗口（transmit window）来处理流量控制。这意味着TCP会根据网络的拥塞情况调整数据的发送速率，以避免网络拥塞和数据丢失。\n",
    "\n",
    "4. **Provides a nice reliable pipe**：\n",
    "   - TCP提供了一个可靠的数据传输通道，确保数据能够完整、按顺序地到达目的地。\n",
    "\n",
    "### 示意图部分：\n",
    "- 图中的堆栈连接示意图展示了TCP在网络协议栈中的位置和作用。\n",
    "- **Application（应用层）**：\n",
    "  - 应用层是用户与网络交互的接口，负责处理特定的网络应用程序。\n",
    "  \n",
    "- **Transport（传输层）**：\n",
    "  - 传输层负责端到端的数据传输，TCP就位于这一层，确保数据的可靠传输。\n",
    "  \n",
    "- **Internet（网络层）**：\n",
    "  - 网络层负责数据包的路由选择和传输，IP协议位于这一层。\n",
    "  \n",
    "- **Link（链路层）**：\n",
    "  - 链路层负责物理网络的传输，包括局域网和广域网的连接。\n",
    "\n",
    "- **Peer-to-peer（对等连接）**：\n",
    "  - 示意图中展示了TCP在两个对等节点之间的连接，确保数据在应用层之间的可靠传输。\n",
    "\n",
    "- **Ethernet（以太网）和Fiber, Satellite, etc.（光纤、卫星等）**：\n",
    "  - 这些是链路层的具体实现方式，负责实际的物理数据传输。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### socket\n",
    "![socket](./assets/socket.png)\n",
    "这张图解释了TCP连接（Transmission Control Protocol，传输控制协议）和套接字（Sockets）的概念。以下是对图中内容的详细解析：\n",
    "\n",
    "1. **标题**：\n",
    "   - \"TCP Connections / Sockets\"：标题表示图的主题是TCP连接和套接字。\n",
    "\n",
    "2. **定义**：\n",
    "   - 图中引用了一段文字，定义了什么是互联网套接字（Internet socket）或网络套接字（network socket）：\n",
    "     - “在计算机网络中，互联网套接字或网络套接字是跨基于互联网协议（Internet Protocol）的计算机网络（如互联网）进行双向进程间通信流的端点。”\n",
    "     - 这段定义说明套接字是用于在网络上（如互联网）进行双向通信的端点。\n",
    "\n",
    "3. **图示**：\n",
    "   - 图示部分展示了两个进程（Process）通过互联网进行通信的方式：\n",
    "     - 左边的方框标注为“Process”，并且手写标注了“Browser”（浏览器），表示这是一个浏览器进程。\n",
    "     - 右边的方框也标注为“Process”，并且手写标注了“Web Server”（网络服务器），表示这是一个网络服务器进程。\n",
    "     - 两个进程通过箭头连接到中间的“Internet”（互联网）云图，表示它们通过互联网进行通信。\n",
    "     - 箭头表示了通信的方向，是双向的。\n",
    "\n",
    "4. **总结**：\n",
    "   - 这张图简要说明了TCP连接和套接字在网络通信中的作用。套接字是两个进程之间通过互联网进行双向通信的端点。一个典型的例子是浏览器（客户端）和网络服务器（服务端）之间的通信。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCP Port\n",
    "![tcpport](./assets/tcpport.png)\n",
    "TCP 端口号\n",
    "- 端口是特定于应用程序或特定于进程的软件通信端点\n",
    "- 它允许多个网络应用程序在同一服务器上共存。\n",
    "- 有一个众所周知的 TCP 端口号列表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http（hypertext transfer protocol）\n",
    "![http](./assets/http.png)\n",
    "根据图片内容，HTTP（超文本传输协议）是互联网中占主导地位的应用层协议。它是为Web发明的，用于检索HTML、图像、文档等内容。HTTP不仅用于文档，还扩展到数据的传输，例如RSS、Web服务等。基本概念包括：\n",
    "\n",
    "1. 建立连接\n",
    "2. 请求文档\n",
    "3. 接收文档\n",
    "4. 关闭连接\n",
    "\n",
    "![socketutf8](./assets/socketutf8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 20 Jul 2024 13:54:28 GMT\n",
      "Server: Apache/2.4.52 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "mysocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "mysocket.connect(('data.pr4e.org',80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysocket.send(cmd)\n",
    "while True:\n",
    "    data = mysocket.recv(512)\n",
    "    if len(data)<1:\n",
    "        break\n",
    "    print(data.decode(),end=\"\")\n",
    "mysocket.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ascii\n",
    "![ascii](./assets/ascii.png)\n",
    "这张图片解释了如何用ASCII码表示简单的字符串。具体内容如下：\n",
    "\n",
    "1. **每个字符的表示**：\n",
    "    - 每个字符都由一个存储在8位内存中的0到256之间的数字表示。\n",
    "\n",
    "2. **字节的定义**：\n",
    "    - 8位内存被称为一个“字节”。\n",
    "    - 比如，一个硬盘的容量可以用字节来表示（例如，3TB的硬盘包含3万亿字节的内存）。\n",
    "\n",
    "3. **ord()函数**：\n",
    "    - `ord()`函数可以告诉我们一个简单ASCII字符的数值。\n",
    "    - 例如：\n",
    "        ```python\n",
    "        print(ord('H'))  # 输出72\n",
    "        print(ord('e'))  # 输出101\n",
    "        print(ord('\\n')) # 输出10\n",
    "        ```\n",
    "\n",
    "这些例子展示了字符'H', 'e'和换行符'\\n'对应的ASCII码值分别是72, 101和10。ASCII码是一种将字符映射到数字的编码方式，广泛用于计算机系统中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多字节字符\n",
    "![multibytecha](./assets/multibytecha.png)\n",
    "这张图片描述了多字符字节（Multi-Byte Characters）的概念，主要介绍了几种常见的字符编码方式及其特点。以下是对图片内容的解析：\n",
    "\n",
    "1. **多字符字节的必要性**：\n",
    "   - 为了表示计算机必须处理的广泛字符范围，我们使用多个字节来表示字符。\n",
    "\n",
    "2. **几种常见的编码方式**：\n",
    "   - **UTF-16**：\n",
    "     - 固定长度：两个字节。\n",
    "   - **UTF-32**：\n",
    "     - 固定长度：四个字节。\n",
    "   - **UTF-8**：\n",
    "     - 可变长度：1-4个字节。\n",
    "     - 向上兼容ASCII。\n",
    "     - 可以在ASCII和UTF-8之间自动检测。\n",
    "     - 推荐作为编码数据在系统之间交换的实践。\n",
    "\n",
    "3. **推荐理由**：\n",
    "   - UTF-8被推荐为编码数据在系统之间交换的实践，因为它兼容ASCII，并且可以自动检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "3\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "x = b'abc'\n",
    "print(type(x))\n",
    "print(len(x))\n",
    "x = 'abc'\n",
    "print(type(x))\n",
    "x = u'abc'\n",
    "print(type(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib\n",
    "`urllib` 是 Python 标准库中用于处理 URL 的模块之一，它提供了一系列用于处理 URL 的功能，包括发送请求、处理响应、解析 URL 等。`urllib` 库通常被用于访问网络资源，如网页、API 等。\n",
    "\n",
    "`urllib` 库主要包含以下几个子模块：\n",
    "\n",
    "1. `urllib.request`: 用于打开和读取 URL。它包含了 `urlopen()` 函数，可以用来发送 HTTP 请求并获取响应。\n",
    "\n",
    "2. `urllib.parse`: 用于解析 URL。它包含了 `urlparse()` 和 `urlunparse()` 等函数，可以解析和构建 URL。\n",
    "\n",
    "3. `urllib.error`: 包含了 `URLError` 和 `HTTPError` 等异常类，用于处理与 URL 相关的错误。\n",
    "\n",
    "4. `urllib.robotparser`: 用于解析 robots.txt 文件，以便遵守网站的爬虫规则。\n",
    "\n",
    "使用 `urllib` 库，你可以实现诸如发送 GET 或 POST 请求、下载文件、解析 URL 等功能。然而，由于 `urllib` 的接口相对较低级，有时候人们更倾向于使用第三方库，如 `requests`，它提供了更简洁和易用的 API 来处理 HTTP 请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0)+1\n",
    "print(counts)\n",
    "fhand.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络爬虫\n",
    "网络爬虫是一种自动化程序，用于在互联网上获取信息。在Python中，有许多流行的网络爬虫包，其中最著名的是Beautiful Soup和Scrapy。\n",
    "\n",
    "1. **Beautiful Soup**:\n",
    "Beautiful Soup是一个用于解析HTML和XML文档的Python库，它能够从网页中提取数据。它提供了简单又方便的方法来浏览、搜索和修改HTML/XML文档的内容。通过Beautiful Soup，用户可以轻松地从网页中提取所需的信息，例如标题、链接、文本等。\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 从网页中提取标题\n",
    "title = soup.title.text\n",
    "print(title)\n",
    "\n",
    "# 从网页中提取所有链接\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))\n",
    "```\n",
    "\n",
    "2. **Scrapy**:\n",
    "Scrapy是一个功能强大的Python网络爬虫框架，它提供了一套完整的工具集，用于快速开发和部署网络爬虫。Scrapy支持并发请求、数据提取、数据存储等功能，使得爬取大规模网站变得更加容易。\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 从网页中提取数据\n",
    "        title = response.css('title::text').get()\n",
    "        print(title)\n",
    "\n",
    "        # 提取链接\n",
    "        links = response.css('a::attr(href)').getall()\n",
    "        for link in links:\n",
    "            print(link)\n",
    "```\n",
    "\n",
    "**网络爬虫的意义**:\n",
    "1. **信息检索**：网络爬虫可以帮助用户从互联网上快速获取所需的信息，节省人力和时间成本。\n",
    "2. **数据分析**：爬虫可以用于收集大量数据，进行数据分析和挖掘，帮助用户做出更好的决策。\n",
    "3. **监控和跟踪**：企业可以利用网络爬虫监控竞争对手的动态，跟踪市场变化，以便及时调整策略。\n",
    "4. **搜索引擎优化**：网站所有者可以利用爬虫来检查网站的SEO情况，优化网站内容以提高搜索引擎排名。\n",
    "5. **内容聚合**：爬虫可以用于从不同网站收集信息，进行内容聚合，为用户提供更全面的信息服务。\n",
    "\n",
    "需要注意的是，在进行网络爬取时，应该遵守网站的robots.txt协议，避免对网站造成不必要的负担，并遵守法律法规，避免侵犯他人的合法权益。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ') # http://www.dr-chuck.com/page1.htm\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "# retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
